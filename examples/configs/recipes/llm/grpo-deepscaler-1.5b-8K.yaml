defaults: ../../grpo_math_1B.yaml
grpo:
  num_prompts_per_step: 128
  num_generations_per_prompt: 8
  max_val_samples: 480
  val_batch_size: 32
  val_period: 500
  max_num_steps: 240
  
loss_fn:
  reference_policy_kl_penalty: 0.0
checkpointing:
  enabled: true
  checkpoint_dir: "results/grpo_deepscaler_1.5b_8k"
  keep_top_k: 10
  model_save_format: null
  save_period: 240
policy:
  model_name: deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B
  train_global_batch_size: 64
  train_micro_batch_size: 4
  max_total_sequence_length: 8192
  dtensor_cfg:
    cpu_offload: true
    activation_checkpointing: true
    _v2: false
  sequence_packing:
    enabled: false
  optimizer:
    kwargs:
      lr: 2.0e-06
  generation:
    vllm_kwargs:
      compilation_config:
        use_inductor: false
data:
  dataset_name: DeepScaler
env:
  math:
    num_workers: 16
logger:
  log_dir: logs_grpo/grpo-deepscaler-1.5b-8k
  num_val_samples_to_print: 0
  wandb_enabled: false
  tensorboard_enabled: true
  mlflow_enabled: false
  swanlab_enabled: false
  monitor_gpus: false
cluster:
  gpus_per_node: 8
