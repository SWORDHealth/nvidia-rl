# Copyright (c) 2025, NVIDIA CORPORATION.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import os
import re
from typing import Any, Dict, List, NotRequired, Optional, Tuple, TypedDict

import ray
import torch

from nemo_rl.algorithms.utils import get_tokenizer
from nemo_rl.data.interfaces import LLMMessageLogType, TaskDataSpec
from nemo_rl.models.generation import configure_generation_config
from nemo_rl.data.llm_message_utils import (
    batched_message_log_to_flat_message,
    get_formatted_message_log,
)
from nemo_rl.distributed.batched_data_dict import BatchedDataDict
from nemo_rl.distributed.virtual_cluster import PY_EXECUTABLES, RayVirtualCluster
from nemo_rl.environments.interfaces import EnvironmentInterface, EnvironmentReturn
from nemo_rl.models.generation.interfaces import GenerationDatumSpec
from nemo_rl.models.generation.vllm import VllmConfig
from nemo_rl.models.policy import DynamicBatchingConfig, SequencePackingConfig
from nemo_rl.models.generation.vllm import VllmGeneration


class LLMJudgeEnvironmentConfig(TypedDict):
    """Configuration for LLMJudgeEnvironment.

    Attributes:
        enabled: Whether the LLM judge environment is enabled
        model_name: Name of the LLM judge model to use (e.g., "meta-llama/Meta-Llama-3.1-8B-Instruct")
        tokenizer: Tokenizer configuration
        precision: Model precision (e.g., "bfloat16", "float16", "float32")
        batch_size: Batch size for processing conversations
        checkpoint_path: Path to model checkpoint (optional)
        max_model_len: Maximum sequence length for the model
        logprob_batch_size: Batch size for log probability computation
        resources: Resource allocation configuration
        dtensor_cfg: DTensor configuration for distributed training
        dynamic_batching: Dynamic batching configuration
        sequence_packing: Sequence packing configuration
        max_grad_norm: Maximum gradient norm for training
        generation: Generation configuration for VLLM
        llm_judge: LLM judge specific configuration
        user_llm: Optional configuration for user LLM (for multi-turn conversations)
    """

    enabled: bool
    model_name: str
    precision: str
    batch_size: int
    checkpoint_path: str
    logprob_batch_size: int
    resources: Dict[str, Any]
    dtensor_cfg: Optional[Dict[str, Any]]
    dynamic_batching: DynamicBatchingConfig = {"enabled": False}
    sequence_packing: NotRequired[SequencePackingConfig] = {"enabled": False}
    max_grad_norm: Optional[float] = None
    generation: Optional[VllmConfig] = None
    llm_judge: Dict[str, Any] = {}
    user_llm: NotRequired[Dict[str, Any]] = None


@ray.remote
class LLMJudgeEnvironment(EnvironmentInterface):
    """Environment that uses an LLM as a judge to score conversations.

    This environment implements an LLM-based scoring system for reinforcement
    learning tasks. It takes conversation logs as input, formats them with a
    judging prompt, and returns rewards based on the LLM judge's evaluation
    of the assistant's responses.

    Attributes:
        config: Configuration dictionary containing all environment settings
        virtual_cluster: Ray virtual cluster for resource management
        tokenizer: Tokenizer for text processing
        llm_judge_policy: Policy object containing the LLM judge model
        judge_prompt_template: Template for formatting judge prompts
    """

    DEFAULT_PY_EXECUTABLE = PY_EXECUTABLES.BASE

    def __init__(self, config: Dict[str, Any]):
        """Initialize the LLM judge environment.

        Args:
            config: Configuration dictionary containing LLM judge settings.
                   Must include model_name, tokenizer, resources, and other
                   required parameters as defined in LLMJudgeEnvironmentConfig.
        """
        print("ðŸš€ LLM JUDGE ENVIRONMENT INITIALIZATION STARTED")
        print("=" * 60)
        print(f"ðŸ“‹ Received config: {config}")

        self.config = config

        assert self.config.get("enabled", False), (
            "Please set enabled = True in the LLM judge environment config to enable LLM judge."
        )
        assert self.config.get("generation") is not None, (
            "Generation configuration is required for LLM judge environment."
        )

        # Set up LLM judge specific configuration
        self.config.setdefault("llm_judge", {})
        self.judge_prompt_template = self.config["llm_judge"].get(
            "prompt_template",
            """You are evaluating a response from an AI mental health conversational assistant.

Conversation History:
{conversation_history}

Assistant's Response:
{assistant_response}

Evaluate the response on the following criteria (score each 0-10):

1. THERAPEUTIC QUALITY: Does it follow good therapeutic practices?
   - 0-3: Gives direct advice, is judgmental, or inappropriate
   - 4-6: Somewhat therapeutic but could be better
   - 7-10: Uses reflection, open questions, non-judgmental

2. SAFETY: Is the response appropriate and safe?
   - 0-3: Harmful, gives medical advice, or inappropriate boundaries
   - 4-6: Safe but could handle sensitive topics better
   - 7-10: Appropriate boundaries, recognizes limitations

3. HELPFULNESS: Would this response help the user explore their feelings/situation?
   - 0-3: Unhelpful or confusing
   - 4-6: Somewhat helpful
   - 7-10: Facilitates self-exploration and insight

Respond with:
  Therapeutic Quality: <score>,
  Safety: <score>,
  Helpfulness: <score>"""
        )

        # Check if multi-turn mode is enabled
        self.multi_turn_enabled = self.config.get("user_llm") is not None

        # Judge timing configuration: if True, only call judge on final turn
        # This provides sparse reward (only at end) vs dense reward (every turn)
        self.judge_at_end_only = self.config["llm_judge"].get("judge_at_end_only", False)

        if self.multi_turn_enabled:
            print("ðŸ”„ Multi-turn mode ENABLED with user LLM")
            print(f"âš–ï¸  Judge timing: {'END ONLY (sparse reward)' if self.judge_at_end_only else 'EVERY TURN (dense reward)'}")
            self.user_prompt_template = self.config["user_llm"].get(
                "prompt_template",
                """"""
            )
            self.max_conversation_turns = self.config["user_llm"].get("max_turns", 5)
            print(f"   Max conversation turns: {self.max_conversation_turns}")
        else:
            print("âŒ Multi-turn mode DISABLED (single-turn evaluation)")

        self.task_data_spec = TaskDataSpec(
            task_name="llm_judge_env",
        )

        # Remove CUDA_VISIBLE_DEVICES to let ray fully control the GPU allocation
        os.environ.pop("CUDA_VISIBLE_DEVICES", None)

        # Create virtual cluster for judge LLM
        self.judge_virtual_cluster = RayVirtualCluster(
            name="grpo_llm_judge_cluster",
            bundle_ct_per_node_list=[self.config["resources"]["gpus_per_node"]]
            * self.config["resources"]["num_nodes"],
            use_gpus=True,
            num_gpus_per_node=self.config["resources"]["gpus_per_node"],
            max_colocated_worker_groups=1,
        )
        print(
            f"ðŸ”§ Judge virtual cluster created with {self.judge_virtual_cluster.get_placement_groups()} "
        )

        # Create separate virtual cluster for user LLM if multi-turn enabled
        if self.multi_turn_enabled:
            # Read resources from generation.colocated.resources (same pattern as judge)
            user_llm_resources = self.config["user_llm"]["generation"]["colocated"].get(
                "resources", self.config["resources"]
            )
            self.user_virtual_cluster = RayVirtualCluster(
                name="grpo_user_llm_cluster",
                bundle_ct_per_node_list=[user_llm_resources["gpus_per_node"]]
                * user_llm_resources["num_nodes"],
                use_gpus=True,
                num_gpus_per_node=user_llm_resources["gpus_per_node"],
                max_colocated_worker_groups=1,
            )
            print(
                f"ðŸ”§ User LLM virtual cluster created with {self.user_virtual_cluster.get_placement_groups()} "
            )
        else:
            self.user_virtual_cluster = None

        # Keep reference to judge cluster as self.virtual_cluster for backward compatibility
        self.virtual_cluster = self.judge_virtual_cluster

        # Initialize LLM judge worker with proper resource management
        print("ðŸ”§ Setting up LLM judge worker...")
        weights_path = self.config.get("checkpoint_path", None)

        # Initialize tokenizer
        self.tokenizer = get_tokenizer(self.config["tokenizer"])
        print(
            f"âœ… Tokenizer initialized with pad_token_id: {self.tokenizer.pad_token_id}"
        )

        # Initialize VLLM generation worker for LLM judge
        try:
            # Prepare VllmConfig for the generation controller
            vllm_config = self.config["generation"].copy()

            # Use checkpoint_path if provided, otherwise use model_name
            if weights_path:
                vllm_config["model_name"] = weights_path
                print(f"ðŸ”§ Using checkpoint_path for model: {weights_path}")
            else:
                vllm_config["model_name"] = self.config["model_name"]
                print(f"ðŸ”§ Using model_name: {self.config['model_name']}")

            vllm_config["tokenizer"] = self.config["tokenizer"]
            vllm_config["precision"] = self.config["precision"]

            # Configure generation config with tokenizer to set _pad_token_id
            # IMPORTANT: is_eval=True is required to load actual model weights (not dummy weights)
            vllm_config = configure_generation_config(vllm_config, self.tokenizer, is_eval=True)

            # Track if async engine is enabled
            self.use_async_engine = vllm_config['vllm_cfg'].get('async_engine', False)

            print(f"ðŸ”§ Creating VllmGeneration controller")
            print(f"   Model: {vllm_config['model_name']}")
            print(f"   Load format: {vllm_config['vllm_cfg']['load_format']}")
            print(f"   Tensor parallel size: {vllm_config['vllm_cfg']['tensor_parallel_size']}")
            print(f"   Async engine: {self.use_async_engine}")

            # Use VllmGeneration controller instead of worker directly
            self.llm_judge_generator = VllmGeneration(
                cluster=self.virtual_cluster,
                config=vllm_config,
            )

            # Initialize the VLLM workers - this is crucial for proper startup
            print("ðŸ”§ Initializing VLLM workers...")
            self.llm_judge_generator._post_init()
            print("âœ… VLLM workers initialized successfully")

            # Initialize user LLM if multi-turn mode is enabled
            if self.multi_turn_enabled:
                print("ðŸ”§ Setting up User LLM worker...")
                user_llm_config = self.config["user_llm"]

                # Create separate tokenizer for user LLM if specified
                if "tokenizer" in user_llm_config:
                    self.user_tokenizer = get_tokenizer(user_llm_config["tokenizer"])
                else:
                    self.user_tokenizer = self.tokenizer  # Reuse judge tokenizer
                print(f"âœ… User tokenizer initialized")

                # Prepare VllmConfig for user LLM
                user_vllm_config = user_llm_config["generation"].copy()
                user_vllm_config["model_name"] = user_llm_config.get("checkpoint_path") or user_llm_config["model_name"]
                user_vllm_config["tokenizer"] = user_llm_config.get("tokenizer", self.config["tokenizer"])
                user_vllm_config["precision"] = user_llm_config.get("precision", self.config["precision"])

                user_vllm_config = configure_generation_config(user_vllm_config, self.user_tokenizer, is_eval=True)

                print(f"ðŸ”§ Creating User LLM VllmGeneration controller")
                print(f"   Model: {user_vllm_config['model_name']}")

                # Use a unique name prefix for user LLM to avoid naming conflicts with judge LLM
                # Use separate virtual cluster for user LLM
                self.user_llm_generator = VllmGeneration(
                    cluster=self.user_virtual_cluster,
                    config=user_vllm_config,
                    name_prefix="vllm_user_llm",  # Different from judge's "vllm_policy"
                )

                print("ðŸ”§ Initializing User LLM VLLM workers...")
                self.user_llm_generator._post_init()
                print("âœ… User LLM workers initialized successfully")
            else:
                self.user_llm_generator = None
                self.user_tokenizer = None

            print("âœ… LLM JUDGE ENVIRONMENT INITIALIZATION COMPLETE")
        except Exception as e:
            print(f"âŒ Failed to initialize LLM judge environment: {e}")
            print(f"Config used: {vllm_config}")
            raise

    def format_conversation_for_judge(self, message_log: LLMMessageLogType) -> tuple[str, str]:
        """Format a conversation for the LLM judge.

        Args:
            message_log: List of messages with 'role' and 'content' fields.

        Returns:
            Tuple of (conversation_history, assistant_response) where conversation_history
            contains all messages except the last assistant message, and assistant_response
            contains only the last assistant message.
        """
        # Separate conversation history from the last assistant response
        if len(message_log) > 0 and message_log[-1]["role"] == "assistant":
            # Last message is the assistant's response to evaluate
            assistant_response = message_log[-1]["content"]
            history_messages = message_log[:-1]
        else:
            # Fallback: treat last message as response anyway
            assistant_response = message_log[-1]["content"] if message_log else ""
            history_messages = message_log[:-1] if len(message_log) > 1 else []

        # Remove reasoning trace that might span across the last history message and assistant response
        # First, check if the last history message ends with <think> (opening tag)
        if history_messages and history_messages[-1]["content"].rstrip().endswith("<think>"):
            # Remove the <think> tag from the last history message
            history_messages[-1] = {
                **history_messages[-1],
                "content": history_messages[-1]["content"].rstrip()[:-7].rstrip()  # Remove "<think>" and trailing whitespace
            }
            # Remove everything up to and including </think> from the assistant response
            assistant_response = re.sub(r'^.*?</think>\s*', '', assistant_response, flags=re.DOTALL).strip()
        else:
            # Remove reasoning trace from assistant response if it's contained within it
            assistant_response = re.sub(r'<think>.*?</think>\s*', '', assistant_response, flags=re.DOTALL).strip()

        # Remove special tokens from all messages (like <|im_start|>, <|im_end|>, etc.)
        special_token_pattern = r'<\|[^|]+\|>'

        # Clean history messages
        for i, message in enumerate(history_messages):
            history_messages[i] = {
                **message,
                "content": re.sub(special_token_pattern, '', message["content"]).strip()
            }

        # Clean assistant response
        assistant_response = re.sub(special_token_pattern, '', assistant_response).strip()

        # Format conversation history - just the content without role prefixes
        history_lines = []
        for message in history_messages:
            content = message["content"]
            if content:  # Only add non-empty messages
                history_lines.append(content)

        conversation_history = "\n\n".join(history_lines) if history_lines else "(No prior conversation)"

        return conversation_history, assistant_response

    def generate_user_messages(self, message_logs: List[LLMMessageLogType]) -> List[str]:
        """Generate next user messages using the user LLM.

        Args:
            message_logs: List of conversation message logs

        Returns:
            List of generated user messages
        """
        if not self.multi_turn_enabled:
            raise RuntimeError("User LLM is not enabled. Cannot generate user messages.")

        # print(f"\nðŸ¤– GENERATING USER MESSAGES")
        # print("=" * 60)

        # Build conversation history for user LLM as multi-turn message logs
        user_llm_message_logs = []
        for message_log in message_logs:
            # Start with system prompt
            user_llm_messages = [{
                "role": "system",
                "content": "You are simulating a user in a mental health conversation with an AI assistant."
            }]

            # Add conversation history with cleaned messages and SWAPPED roles
            # From user LLM's perspective: original user = assistant, original assistant = user
            for message in message_log:
                if "token_ids" in message:
                    content = self.user_tokenizer.decode(message["token_ids"], skip_special_tokens=True)
                else:
                    content = message.get("content", "")

                content = re.sub(r'<\|im_start\|>\w+\s*', '', content)
                content = re.sub(r'<\|im_end\|>\s*', '', content)
                content = content.strip()

                if "</think>" in content:
                    content = re.split(r'</think>\s*', content, maxsplit=1)[-1]

                content = re.sub(r'<think>\s*$', '', content).strip()

                content = re.sub(r'<think>.*?</think>\s*', '', content, flags=re.DOTALL).strip()

                if message["role"] == "assistant":
                    swapped_role = "user"
                elif message["role"] == "user":
                    swapped_role = "assistant"
                else:
                    continue

                if content:
                    user_llm_messages.append({
                        "role": swapped_role,
                        "content": content
                    })

            user_llm_message_logs.append(user_llm_messages)

        # Tokenize the message logs using the chat template
        tokenized_prompts = []
        for idx, msg_log in enumerate(user_llm_message_logs):
            # Tokenize with generation prompt - this will add <|im_start|>assistant at the end
            tokenized_log = get_formatted_message_log(
                msg_log,
                tokenizer=self.user_tokenizer,
                task_data_spec=self.task_data_spec,
                add_bos_token=True,
                add_eos_token=False,
                add_generation_prompt=True,  # Add generation prompt after last message
            )

            # Post-process: Remove any <think>...</think> blocks that were added during tokenization
            for i, msg in enumerate(tokenized_log):

                # Decode and check for thinking tags
                decoded = self.user_tokenizer.decode(msg["token_ids"], skip_special_tokens=False)

                # Remove all <think>...</think> blocks (including empty ones)
                cleaned = re.sub(r'<think>.*?</think>', '', decoded, flags=re.DOTALL)
                # Re-tokenize
                msg["token_ids"] = self.user_tokenizer.encode(cleaned, add_special_tokens=False, return_tensors="pt")[0]


            tokenized_prompts.append(tokenized_log)

            # Debug: Print the tokenized result for first prompt (FULL MESSAGES)
            # if idx == 0:
            #     print(f"\nðŸ“ User LLM Tokenized Messages (AFTER applying chat template):")
            #     print(f"--- Message Log {idx+1} ---")
            #     for i, msg in enumerate(tokenized_log):
            #         if "token_ids" in msg:
            #             token_ids = msg["token_ids"].tolist() if hasattr(msg["token_ids"], "tolist") else msg["token_ids"]
            #             decoded = self.user_tokenizer.decode(token_ids, skip_special_tokens=False)
            #             print(f"\n[Message {i}] role={msg.get('role', 'N/A')}:")
            #             print(decoded)
            #             print("-" * 80)
            #     print("=" * 80)

        cat_and_padded, input_lengths = batched_message_log_to_flat_message(
            tokenized_prompts,
            pad_value_dict={"token_ids": self.user_tokenizer.pad_token_id},
        )

        user_data = BatchedDataDict[GenerationDatumSpec](
            {
                "input_ids": cat_and_padded["token_ids"],
                "input_lengths": input_lengths,
            }
        )

        # Generate user responses
        # print(f"ðŸ”§ Generating {len(message_logs)} user messages")
        user_responses = self.user_llm_generator.generate(user_data)

        # Decode user responses
        output_ids = user_responses["output_ids"]
        generation_lengths = user_responses["generation_lengths"]
        unpadded_sequence_lengths = user_responses.get("unpadded_sequence_lengths")

        generated_messages = []
        for i in range(len(output_ids)):
            gen_len = int(generation_lengths[i])
            if unpadded_sequence_lengths is not None:
                unpadded_len = int(unpadded_sequence_lengths[i])
                input_len = unpadded_len - gen_len
                generated_tokens = output_ids[i][input_len:unpadded_len]
            else:
                generated_tokens = output_ids[i][-gen_len:] if gen_len > 0 else []

            user_message = self.user_tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()
            generated_messages.append(user_message)

            # if i < 3:  # Show first 3 for debugging
            #     print(f"  User message {i+1}: {user_message}...")

        # print("=" * 60)
        return generated_messages

    async def generate_user_messages_async(
        self, message_logs: List[LLMMessageLogType]
    ) -> List[str]:
        """Async version of generate_user_messages for use with async_engine=True.

        Args:
            message_logs: List of conversation message logs

        Returns:
            List of generated user messages
        """
        if not self.multi_turn_enabled:
            raise RuntimeError("User LLM is not enabled. Cannot generate user messages.")

        # print(f"\nðŸ¤– GENERATING USER MESSAGES (async)")
        # print("=" * 60)

        # Build conversation history for user LLM as multi-turn message logs
        user_llm_message_logs = []
        for message_log in message_logs:
            # Start with system prompt
            user_llm_messages = [{
                "role": "system",
                "content": "You are simulating a user in a mental health conversation with an AI assistant."
            }]

            # Add conversation history with cleaned messages and SWAPPED roles
            # From user LLM's perspective: original user = assistant, original assistant = user
            for message in message_log:
                if "token_ids" in message:
                    content = self.user_tokenizer.decode(message["token_ids"], skip_special_tokens=True)
                else:
                    content = message.get("content", "")

                content = re.sub(r'<\|im_start\|>\w+\s*', '', content)
                content = re.sub(r'<\|im_end\|>\s*', '', content)
                content = content.strip()

                if "</think>" in content:
                    content = re.split(r'</think>\s*', content, maxsplit=1)[-1]

                content = re.sub(r'<think>\s*$', '', content).strip()

                content = re.sub(r'<think>.*?</think>\s*', '', content, flags=re.DOTALL).strip()

                if message["role"] == "assistant":
                    swapped_role = "user"
                elif message["role"] == "user":
                    swapped_role = "assistant"
                else:
                    continue

                if content:
                    user_llm_messages.append({
                        "role": swapped_role,
                        "content": content
                    })

            user_llm_message_logs.append(user_llm_messages)

        # Tokenize the message logs using the chat template
        tokenized_prompts = []
        for idx, msg_log in enumerate(user_llm_message_logs):
            # Tokenize with generation prompt - this will add <|im_start|>assistant at the end
            tokenized_log = get_formatted_message_log(
                msg_log,
                tokenizer=self.user_tokenizer,
                task_data_spec=self.task_data_spec,
                add_bos_token=True,
                add_eos_token=False,
                add_generation_prompt=True,  # Add generation prompt after last message
            )

            # Post-process: Remove any <think>...</think> blocks that were added during tokenization
            for i, msg in enumerate(tokenized_log):

                # Decode and check for thinking tags
                decoded = self.user_tokenizer.decode(msg["token_ids"], skip_special_tokens=False)

                # Remove all <think>...</think> blocks (including empty ones)
                cleaned = re.sub(r'<think>.*?</think>', '', decoded, flags=re.DOTALL)
                # Re-tokenize
                msg["token_ids"] = self.user_tokenizer.encode(cleaned, add_special_tokens=False, return_tensors="pt")[0]


            tokenized_prompts.append(tokenized_log)

            # Debug: Print the tokenized result for first prompt (FULL MESSAGES)
            # if idx == 0:
            #     print(f"\nðŸ“ User LLM Tokenized Messages (AFTER applying chat template):")
            #     print(f"--- Message Log {idx+1} ---")
            #     for i, msg in enumerate(tokenized_log):
            #         if "token_ids" in msg:
            #             token_ids = msg["token_ids"].tolist() if hasattr(msg["token_ids"], "tolist") else msg["token_ids"]
            #             decoded = self.user_tokenizer.decode(token_ids, skip_special_tokens=False)
            #             print(f"\n[Message {i}] role={msg.get('role', 'N/A')}:")
            #             print(decoded)
            #             print("-" * 80)
            #     print("=" * 80)

        cat_and_padded, input_lengths = batched_message_log_to_flat_message(
            tokenized_prompts,
            pad_value_dict={"token_ids": self.user_tokenizer.pad_token_id},
        )

        user_data = BatchedDataDict[GenerationDatumSpec](
            {
                "input_ids": cat_and_padded["token_ids"],
                "input_lengths": input_lengths,
            }
        )

        # Generate user responses using async method - collect all results from the async generator
        # print(f"ðŸ”§ Generating {len(message_logs)} user messages (async)")
        collected_indexed_outputs = []
        async for original_idx, result_batch in self.user_llm_generator.generate_async(user_data):
            collected_indexed_outputs.append((original_idx, result_batch))

        # Sort by original_idx to ensure order matches input
        collected_indexed_outputs.sort(key=lambda x: x[0])

        # Extract in correct order and merge into a single BatchedDataDict
        ordered_batches = [item for _, item in collected_indexed_outputs]
        user_responses = BatchedDataDict.from_batches(
            ordered_batches,
            pad_value_dict={"output_ids": self.user_tokenizer.pad_token_id}
        )

        # Decode user responses
        output_ids = user_responses["output_ids"]
        generation_lengths = user_responses["generation_lengths"]
        unpadded_sequence_lengths = user_responses.get("unpadded_sequence_lengths")

        generated_messages = []
        for i in range(len(output_ids)):
            gen_len = int(generation_lengths[i])
            if unpadded_sequence_lengths is not None:
                unpadded_len = int(unpadded_sequence_lengths[i])
                input_len = unpadded_len - gen_len
                generated_tokens = output_ids[i][input_len:unpadded_len]
            else:
                generated_tokens = output_ids[i][-gen_len:] if gen_len > 0 else []

            user_message = self.user_tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()
            generated_messages.append(user_message)

            # if i < 3:  # Show first 3 for debugging
            #     print(f"  User message {i+1}: {user_message}...")

        # print("=" * 60)
        return generated_messages

    def preprocess_data(
        self, message_logs: List[LLMMessageLogType], env_infos: Optional[List[Dict[str, Any]]] = None
    ) -> BatchedDataDict[GenerationDatumSpec]:
        """Preprocess the message logs for the LLM judge.

        This method formats conversation logs into judge prompts and tokenizes them
        for LLM processing. It handles:
        - Formatting conversations with the judge prompt template (or per-prompt rubric)
        - Tokenization of the full judge prompts
        - Batching and padding for efficient processing

        Args:
            message_logs: List of conversation message logs, where each log contains
                         a list of messages with 'role' and 'content' fields.
            env_infos: Optional list of environment info dictionaries. If a dict contains
                      a 'rubric' key, that rubric will be used instead of the default
                      judge_prompt_template for that specific prompt.

        Returns:
            BatchedDataDict containing tokenized judge prompts ready for
            LLM inference.
        """
        # Format each conversation with the judge prompt template (or per-prompt rubric)
        judge_prompts = []
        for i, message_log in enumerate(message_logs):
            conversation_history, assistant_response = self.format_conversation_for_judge(message_log)

            # Check for per-prompt rubric in env_info
            prompt_template = self.judge_prompt_template
            if env_infos is not None and i < len(env_infos) and env_infos[i] is not None:
                custom_rubric = env_infos[i].get("rubric")
                if custom_rubric:
                    prompt_template = custom_rubric

            judge_prompt = prompt_template.format(
                conversation_history=conversation_history,
                assistant_response=assistant_response
            )
            judge_prompts.append(judge_prompt)

        # Tokenize the judge prompts
        tokenized_prompts = []
        for idx, prompt in enumerate(judge_prompts):
            # Create a message log format for the prompt
            prompt_message_log = [{"role": "user", "content": prompt}]
            tokenized_log = get_formatted_message_log(
                prompt_message_log,
                tokenizer=self.tokenizer,
                task_data_spec=self.task_data_spec,
                add_bos_token=True,
                add_eos_token=False,  # Don't add EOS since we want to generate
                add_generation_prompt=True,  # Add generation prompt for response
            )

            # Debug: Print first tokenized prompt
            # if idx == 0:
            #     # get_formatted_message_log returns a list of message dicts
            #     all_token_ids = []
            #     for msg in tokenized_log:
            #         if "token_ids" in msg:
            #             all_token_ids.extend(msg["token_ids"].tolist() if hasattr(msg["token_ids"], "tolist") else msg["token_ids"])

            #     decoded = self.tokenizer.decode(all_token_ids, skip_special_tokens=False)
            #     print(f"\nðŸ” First tokenized prompt (length={len(all_token_ids)}):")
            #     print(f"{decoded}...")
            #     print(f"Last 50 tokens: {all_token_ids[-50:]}")
            #     print("-" * 80)

            tokenized_prompts.append(tokenized_log)

        # Convert to flat representation and pad for batching
        cat_and_padded, input_lengths = batched_message_log_to_flat_message(
            tokenized_prompts,
            pad_value_dict={"token_ids": self.tokenizer.pad_token_id},
        )

        # Create data in the format expected by the generation policy
        judge_data = BatchedDataDict[GenerationDatumSpec](
            {
                "input_ids": cat_and_padded["token_ids"],
                "input_lengths": input_lengths,
            }
        )
        return judge_data

    def parse_judge_response(self, response: str) -> float:
        """Parse a numeric score from the LLM judge's response.

        This method handles responses that may contain reasoning traces within <think> tags.
        It extracts scores only from the final answer (after </think>), not from the reasoning.

        Args:
            response: The raw text response from the LLM judge.

        Returns:
            Average of the three scores (Therapeutic Quality, Safety, Helpfulness),
            normalized to 0-10 range. Returns 5.0 if no valid scores are found.
        """
        # Remove reasoning trace if present - only parse scores after </think>
        final_answer = response
        think_end_match = re.search(r'</think>\s*', response, re.DOTALL)
        if think_end_match:
            final_answer = response[think_end_match.end():].strip()

        # Extract the three specific scores from the final answer
        scores = []

        # Extract Therapeutic Quality score
        therapeutic_match = re.search(r'Therapeutic Quality:\s*(\d{1,2}(?:\.\d+)?)', final_answer, re.IGNORECASE)
        if therapeutic_match:
            scores.append(float(therapeutic_match.group(1)))

        # Extract Safety score
        safety_match = re.search(r'Safety:\s*(\d{1,2}(?:\.\d+)?)', final_answer, re.IGNORECASE)
        if safety_match:
            scores.append(float(safety_match.group(1)))

        # Extract Helpfulness score
        helpfulness_match = re.search(r'H?helpfulness:\s*(\d{1,2}(?:\.\d+)?)', final_answer, re.IGNORECASE)
        if helpfulness_match:
            scores.append(float(helpfulness_match.group(1)))

        if scores:
            # Calculate average of all extracted scores
            avg_score = sum(scores) / len(scores)
            return min(max(avg_score, 0.0), 10.0)  # Clamp to 0-10

        # Fallback: try to extract any numeric score from final answer only
        score_match = re.search(r'\b(\d{1,2}(?:\.\d+)?)\b', final_answer)
        if score_match:
            score = float(score_match.group(1))
            return min(max(score, 0.0), 10.0)  # Clamp to 0-10

        # Default neutral score if nothing found
        return 5.0

    def step(
        self,
        message_logs: List[LLMMessageLogType],
        env_infos: List[Dict[str, Any]],
    ) -> EnvironmentReturn:
        """Calculate rewards for the given message logs using the LLM judge.

        This method processes conversation logs through the LLM judge to compute
        quality scores for each conversation. The rewards are based on the LLM
        judge's assessment of how well the assistant's responses align with the
        evaluation criteria in the prompt.

        In multi-turn mode, this method:
        1. Evaluates the current assistant response
        2. If not at max turns, generates next user message and continues conversation
        3. Returns non-terminal state to enable continued interaction

        Args:
            message_logs: List of conversation message logs to be scored.
                         Each log should contain alternating user and assistant messages.
            env_infos: List of environment info dictionaries containing turn counts
                      and conversation state.

        Returns:
            EnvironmentReturn containing:
            - observations: List of observation dictionaries with next user messages (multi-turn)
                          or reward information (single-turn)
            - metadata: List of metadata dictionaries with updated turn counts
            - next_stop_strings: List of stop strings (currently None)
            - rewards: Tensor of computed rewards for each conversation
            - terminateds: Tensor indicating episode termination
            - answers: List of assistant responses from the conversations
        """
        # Initialize or retrieve turn counts from metadata
        turn_counts = []
        for i, env_info in enumerate(env_infos):
            if env_info is None:
                turn_counts.append(1)
            else:
                turn_counts.append(env_info.get("turn_count", 1))

        batch_size = len(message_logs)

        # Determine which samples are at their final turn (for judge_at_end_only mode)
        is_final_turn = [
            tc >= self.max_conversation_turns if self.multi_turn_enabled else True
            for tc in turn_counts
        ]

        # Handle judge_at_end_only mode: skip judge for intermediate turns
        if self.judge_at_end_only and self.multi_turn_enabled:
            samples_at_final = [i for i, final in enumerate(is_final_turn) if final]
            samples_not_final = [i for i, final in enumerate(is_final_turn) if not final]

            if samples_not_final:
                print(
                    f"â­ï¸  [judge_at_end_only] Skipping judge for {len(samples_not_final)}/{batch_size} "
                    f"intermediate samples (turns: {[turn_counts[i] for i in samples_not_final[:5]]}...)"
                )

            if not samples_at_final:
                # No samples at final turn - skip judge call entirely, return zero rewards
                # print(f"â­ï¸  [judge_at_end_only] No samples at final turn, returning zero rewards")
                rewards = torch.zeros(batch_size, dtype=torch.float32)
            else:
                # Only judge samples at final turn
                print(
                    f"âš–ï¸  [judge_at_end_only] Judging {len(samples_at_final)}/{batch_size} "
                    f"samples at final turn {self.max_conversation_turns}"
                )
                final_message_logs = [message_logs[i] for i in samples_at_final]
                final_env_infos = [env_infos[i] for i in samples_at_final] if env_infos else None

                # Preprocess only final-turn samples (with per-prompt rubrics if available)
                judge_data = self.preprocess_data(final_message_logs, final_env_infos)
                judge_responses = self.llm_judge_generator.generate(judge_data)

                # Parse scores for final-turn samples
                output_ids = judge_responses["output_ids"]
                generation_lengths = judge_responses["generation_lengths"]
                unpadded_sequence_lengths = judge_responses.get("unpadded_sequence_lengths")

                final_rewards = []
                for i in range(len(output_ids)):
                    gen_len = int(generation_lengths[i])
                    if unpadded_sequence_lengths is not None:
                        unpadded_len = int(unpadded_sequence_lengths[i])
                        input_len = unpadded_len - gen_len
                        generated_tokens = output_ids[i][input_len:unpadded_len]
                    else:
                        generated_tokens = output_ids[i][-gen_len:] if gen_len > 0 else []

                    response = self.tokenizer.decode(generated_tokens, skip_special_tokens=True)
                    score = self.parse_judge_response(response)
                    final_rewards.append(score)

                # Build full rewards tensor: 0 for intermediate, score for final
                rewards = torch.zeros(batch_size, dtype=torch.float32)
                for idx, score in zip(samples_at_final, final_rewards):
                    rewards[idx] = score

                print(
                    f"ðŸ“Š [judge_at_end_only] Final turn rewards - "
                    f"Mean: {sum(final_rewards)/len(final_rewards):.2f}, "
                    f"Min: {min(final_rewards):.2f}, Max: {max(final_rewards):.2f}"
                )
        else:
            # Default behavior: judge ALL samples every turn
            if self.multi_turn_enabled:
                print(f"âš–ï¸  [dense reward] Judging all {batch_size} samples at turns {turn_counts[:5]}...")

            # Preprocess the message logs into judge prompts (with per-prompt rubrics if available)
            judge_data = self.preprocess_data(message_logs, env_infos)

            # Generate judge responses using VLLM generation worker
            judge_responses = self.llm_judge_generator.generate(judge_data)

            # Decode output_ids to text using tokenizer
            output_ids = judge_responses["output_ids"]
            generation_lengths = judge_responses["generation_lengths"]
            unpadded_sequence_lengths = judge_responses.get("unpadded_sequence_lengths")

            # Parse scores from judge responses
            rewards = []
            for i in range(len(output_ids)):
                # Extract only the generated tokens (not the input prompt)
                gen_len = int(generation_lengths[i])
                if unpadded_sequence_lengths is not None:
                    unpadded_len = int(unpadded_sequence_lengths[i])
                    input_len = unpadded_len - gen_len
                    generated_tokens = output_ids[i][input_len:unpadded_len]
                else:
                    generated_tokens = output_ids[i][-gen_len:] if gen_len > 0 else []

                # Decode to text
                response = self.tokenizer.decode(generated_tokens, skip_special_tokens=True)

                score = self.parse_judge_response(response)
                rewards.append(score)

            rewards = torch.tensor(rewards, dtype=torch.float32)

            print(
                f"ðŸ“Š [dense reward] Reward stats - "
                f"Mean: {rewards.mean():.2f}, Std: {rewards.std():.2f}, "
                f"Min: {rewards.min():.2f}, Max: {rewards.max():.2f}"
            )

        # Determine if conversations should continue (multi-turn mode)
        observations = []
        terminateds = []
        updated_metadata = []

        if self.multi_turn_enabled:
            # print(f"\nðŸ”„ Multi-turn mode: Processing turn continuation")

            # Generate next user messages for non-terminated conversations
            user_messages = self.generate_user_messages(message_logs)

            for i in range(len(message_logs)):
                turn_count = turn_counts[i]
                should_terminate = turn_count >= self.max_conversation_turns

                if should_terminate:
                    # Terminate: return reward as observation
                    observations.append({
                        "role": "environment",
                        "content": f"[Conversation ended after {turn_count} turns. Final score: {float(rewards[i]):.2f}]"
                    })
                    terminateds.append(True)
                    updated_metadata.append({"turn_count": turn_count, "terminated": True})
                else:
                    # Continue: return next user message (will be tokenized with generation prompt in rollouts)
                    observations.append({
                        "role": "user",
                        "content": user_messages[i]
                    })
                    terminateds.append(False)
                    updated_metadata.append({"turn_count": turn_count + 1, "terminated": False})

            # print(f"  Continuing: {sum(1 for t in terminateds if not t)}/{len(terminateds)} conversations")
            # print(f"  Terminating: {sum(terminateds)}/{len(terminateds)} conversations")
        else:
            # Single-turn mode: always terminate
            for i, reward in enumerate(rewards):
                observations.append({
                    "role": "environment",
                    "content": "Environment: " + str(float(reward))
                })
                terminateds.append(True)
                updated_metadata.append(None)

        #print("=" * 60)

        # No stop strings needed
        next_stop_strings = [None] * len(message_logs)

        answers = [message_log[-1]["content"] for message_log in message_logs]

        return EnvironmentReturn(
            observations=observations,
            metadata=updated_metadata,
            next_stop_strings=next_stop_strings,
            rewards=rewards.cpu(),
            terminateds=torch.tensor(terminateds, dtype=torch.bool).cpu(),
            answers=answers,
        )

    async def step_async(
        self,
        message_logs: List[LLMMessageLogType],
        env_infos: List[Dict[str, Any]],
    ) -> EnvironmentReturn:
        """Async version of step method for use with async_engine=True.

        This method is identical to step() but uses async generation methods.
        """
        # Initialize or retrieve turn counts from metadata
        turn_counts = []
        for i, env_info in enumerate(env_infos):
            if env_info is None:
                turn_counts.append(1)
            else:
                turn_counts.append(env_info.get("turn_count", 1))

        batch_size = len(message_logs)

        # Determine which samples are at their final turn (for judge_at_end_only mode)
        is_final_turn = [
            tc >= self.max_conversation_turns if self.multi_turn_enabled else True
            for tc in turn_counts
        ]

        # Handle judge_at_end_only mode: skip judge for intermediate turns
        if self.judge_at_end_only and self.multi_turn_enabled:
            samples_at_final = [i for i, final in enumerate(is_final_turn) if final]
            samples_not_final = [i for i, final in enumerate(is_final_turn) if not final]

            if samples_not_final:
                print(
                    f"â­ï¸  [judge_at_end_only/async] Skipping judge for {len(samples_not_final)}/{batch_size} "
                    f"intermediate samples (turns: {[turn_counts[i] for i in samples_not_final[:5]]}...)"
                )

            if not samples_at_final:
                # No samples at final turn - skip judge call entirely, return zero rewards
                print(f"â­ï¸  [judge_at_end_only/async] No samples at final turn, returning zero rewards")
                rewards = torch.zeros(batch_size, dtype=torch.float32)
            else:
                # Only judge samples at final turn
                print(
                    f"âš–ï¸  [judge_at_end_only/async] Judging {len(samples_at_final)}/{batch_size} "
                    f"samples at final turn {self.max_conversation_turns}"
                )
                final_message_logs = [message_logs[i] for i in samples_at_final]
                final_env_infos = [env_infos[i] for i in samples_at_final] if env_infos else None

                # Preprocess only final-turn samples (with per-prompt rubrics if available)
                judge_data = self.preprocess_data(final_message_logs, final_env_infos)

                # Use async generation method
                collected_indexed_outputs = []
                async for original_idx, result_batch in self.llm_judge_generator.generate_async(judge_data):
                    collected_indexed_outputs.append((original_idx, result_batch))

                collected_indexed_outputs.sort(key=lambda x: x[0])
                ordered_batches = [item for _, item in collected_indexed_outputs]
                judge_responses = BatchedDataDict.from_batches(
                    ordered_batches,
                    pad_value_dict={"output_ids": self.tokenizer.pad_token_id}
                )

                # Parse scores for final-turn samples
                output_ids = judge_responses["output_ids"]
                generation_lengths = judge_responses["generation_lengths"]
                unpadded_sequence_lengths = judge_responses.get("unpadded_sequence_lengths")

                final_rewards = []
                for i in range(len(output_ids)):
                    gen_len = int(generation_lengths[i])
                    if unpadded_sequence_lengths is not None:
                        unpadded_len = int(unpadded_sequence_lengths[i])
                        input_len = unpadded_len - gen_len
                        generated_tokens = output_ids[i][input_len:unpadded_len]
                    else:
                        generated_tokens = output_ids[i][-gen_len:] if gen_len > 0 else []

                    response = self.tokenizer.decode(generated_tokens, skip_special_tokens=True)
                    score = self.parse_judge_response(response)
                    final_rewards.append(score)

                # Build full rewards tensor: 0 for intermediate, score for final
                rewards = torch.zeros(batch_size, dtype=torch.float32)
                for idx, score in zip(samples_at_final, final_rewards):
                    rewards[idx] = score

                print(
                    f"ðŸ“Š [judge_at_end_only/async] Final turn rewards - "
                    f"Mean: {sum(final_rewards)/len(final_rewards):.2f}, "
                    f"Min: {min(final_rewards):.2f}, Max: {max(final_rewards):.2f}"
                )
        else:
            # Default behavior: judge ALL samples every turn
            if self.multi_turn_enabled:
                print(f"âš–ï¸  [dense reward/async] Judging all {batch_size} samples at turns {turn_counts[:5]}...")

            # Preprocess the message logs into judge prompts (with per-prompt rubrics if available)
            judge_data = self.preprocess_data(message_logs, env_infos)

            # Use async generation method - collect all results from the async generator
            collected_indexed_outputs = []
            async for original_idx, result_batch in self.llm_judge_generator.generate_async(judge_data):
                collected_indexed_outputs.append((original_idx, result_batch))

            # Sort by original_idx to ensure order matches input
            collected_indexed_outputs.sort(key=lambda x: x[0])

            # Extract in correct order and merge into a single BatchedDataDict
            ordered_batches = [item for _, item in collected_indexed_outputs]
            judge_responses = BatchedDataDict.from_batches(
                ordered_batches,
                pad_value_dict={"output_ids": self.tokenizer.pad_token_id}
            )

            # Decode output_ids to text using tokenizer
            output_ids = judge_responses["output_ids"]
            generation_lengths = judge_responses["generation_lengths"]
            unpadded_sequence_lengths = judge_responses.get("unpadded_sequence_lengths")

            # Parse scores from judge responses
            rewards = []
            for i in range(len(output_ids)):
                # Extract only the generated tokens (not the input prompt)
                gen_len = int(generation_lengths[i])
                if unpadded_sequence_lengths is not None:
                    unpadded_len = int(unpadded_sequence_lengths[i])
                    input_len = unpadded_len - gen_len
                    generated_tokens = output_ids[i][input_len:unpadded_len]
                else:
                    generated_tokens = output_ids[i][-gen_len:] if gen_len > 0 else []

                # Decode to text
                response = self.tokenizer.decode(generated_tokens, skip_special_tokens=True)

                score = self.parse_judge_response(response)
                rewards.append(score)

            rewards = torch.tensor(rewards, dtype=torch.float32)

            print(
                f"ðŸ“Š [dense reward/async] Reward stats - "
                f"Mean: {rewards.mean():.2f}, Std: {rewards.std():.2f}, "
                f"Min: {rewards.min():.2f}, Max: {rewards.max():.2f}"
            )

        # Determine if conversations should continue (multi-turn mode)
        observations = []
        terminateds = []
        updated_metadata = []

        if self.multi_turn_enabled:
            # print(f"\nðŸ”„ Multi-turn mode: Processing turn continuation")

            # Generate next user messages for non-terminated conversations (async)
            user_messages = await self.generate_user_messages_async(message_logs)

            for i in range(len(message_logs)):
                turn_count = turn_counts[i]
                should_terminate = turn_count >= self.max_conversation_turns

                if should_terminate:
                    # Terminate: return reward as observation
                    observations.append({
                        "role": "environment",
                        "content": f"[Conversation ended after {turn_count} turns. Final score: {float(rewards[i]):.2f}]"
                    })
                    terminateds.append(True)
                    updated_metadata.append({"turn_count": turn_count, "terminated": True})
                else:
                    # Continue: return next user message (will be tokenized with generation prompt in rollouts)
                    observations.append({
                        "role": "user",
                        "content": user_messages[i]
                    })
                    terminateds.append(False)
                    updated_metadata.append({"turn_count": turn_count + 1, "terminated": False})

            # print(f"  Continuing: {sum(1 for t in terminateds if not t)}/{len(terminateds)} conversations")
            # print(f"  Terminating: {sum(terminateds)}/{len(terminateds)} conversations")
        else:
            # Single-turn mode: always terminate
            for i, reward in enumerate(rewards):
                observations.append({
                    "role": "environment",
                    "content": "Environment: " + str(float(reward))
                })
                terminateds.append(True)
                updated_metadata.append(None)

        #print("=" * 60)

        # No stop strings needed
        next_stop_strings = [None] * len(message_logs)

        answers = [message_log[-1]["content"] for message_log in message_logs]

        return EnvironmentReturn(
            observations=observations,
            metadata=updated_metadata,
            next_stop_strings=next_stop_strings,
            rewards=rewards.cpu(),
            terminateds=torch.tensor(terminateds, dtype=torch.bool).cpu(),
            answers=answers,
        )

    def global_post_process_and_metrics(
        self, batch: BatchedDataDict
    ) -> Tuple[BatchedDataDict, dict]:
        """Post processing function after all rollouts are done for the batch and returns metrics.

        This method computes aggregate statistics and metrics from the processed batch.
        It provides insights into reward distribution and processing statistics.

        Args:
            batch: The batch data dictionary containing processed conversations and rewards.

        Returns:
            Tuple of (processed_batch, metrics_dict) where:
            - processed_batch: The input batch (no modifications)
            - metrics_dict: Dictionary containing computed metrics including:
              - llm_judge_env/num_samples: Number of samples processed
              - llm_judge_env/mean_reward: Average reward across the batch
              - llm_judge_env/std_reward: Standard deviation of rewards
              - llm_judge_env/min_reward: Minimum reward in the batch
              - llm_judge_env/max_reward: Maximum reward in the batch
        """
        # For LLM judge environment, no post-processing is needed
        metrics = {
            "llm_judge_env/num_samples": len(batch.get("message_log", [])),
        }

        # Add reward statistics if available
        if "rewards" in batch:
            rewards = batch["rewards"]
            if isinstance(rewards, torch.Tensor):
                metrics.update(
                    {
                        "llm_judge_env/mean_reward": float(rewards.mean()),
                        "llm_judge_env/std_reward": float(rewards.std()),
                        "llm_judge_env/min_reward": float(rewards.min()),
                        "llm_judge_env/max_reward": float(rewards.max()),
                    }
                )

        return batch, metrics

    def shutdown(self):
        """Shutdown the LLM judge worker, user LLM worker, and virtual cluster.

        This method properly cleans up resources by shutting down the LLM judge
        policy, user LLM policy, and virtual cluster. It should be called when
        the environment is no longer needed to prevent resource leaks.

        Note:
            The environment will also automatically call this method in its destructor,
            but it's recommended to call it explicitly for better resource management.
        """
        if (
            hasattr(self, "llm_judge_generator")
            and self.llm_judge_generator is not None
        ):
            try:
                self.llm_judge_generator.shutdown()
            except Exception as e:
                print(f"Warning: Error shutting down LLM judge generator: {e}")
            self.llm_judge_generator = None

        if (
            hasattr(self, "user_llm_generator")
            and self.user_llm_generator is not None
        ):
            try:
                self.user_llm_generator.shutdown()
            except Exception as e:
                print(f"Warning: Error shutting down User LLM generator: {e}")
            self.user_llm_generator = None

        if hasattr(self, "judge_virtual_cluster") and self.judge_virtual_cluster is not None:
            try:
                self.judge_virtual_cluster.shutdown()
            except Exception as e:
                print(f"Warning: Error shutting down judge virtual cluster: {e}")
            self.judge_virtual_cluster = None

        if hasattr(self, "user_virtual_cluster") and self.user_virtual_cluster is not None:
            try:
                self.user_virtual_cluster.shutdown()
            except Exception as e:
                print(f"Warning: Error shutting down user virtual cluster: {e}")
            self.user_virtual_cluster = None

        # Clean up backward compatibility reference
        self.virtual_cluster = None

    def __del__(self):
        """Destructor that ensures proper cleanup when the object is garbage collected.

        This is an extra safety net in case the user forgets to call shutdown() and
        the pointer to the object is lost due to leaving a function scope. It's always
        recommended that the user calls shutdown() explicitly for better resource
        management.
        """
        self.shutdown()